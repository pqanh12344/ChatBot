import streamlit as st
import os
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
import numpy as np
import gc
import faiss
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import re
import logging
import torch
from typing import List, Tuple
import time
import PyPDF2

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# H√†m x·ª≠ l√Ω vƒÉn b·∫£n
def preprocess_text(text: str) -> str:
    try:
        return re.sub(r'\s+', ' ', text).strip()
    except Exception as e:
        logger.error(f"Error in preprocess_text: {e}")
        return text

# T·∫£i d·ªØ li·ªáu t·ª´ PDF v·ªõi cache
@st.cache_data
def load_documents(pdf_path: str) -> List[str]:
    try:
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"File not found: {pdf_path}")
        contexts = []
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            for page in pdf_reader.pages:
                text = page.extract_text()
                if text:
                    contexts.append(preprocess_text(text))
        if not contexts:
            raise ValueError("No text extracted from PDF")
        return contexts
    except Exception as e:
        logger.error(f"Error loading PDF: {e}")
        raise

# Chia nh·ªè vƒÉn b·∫£n v·ªõi cache
@st.cache_data
def chunk_documents(contexts: List[str], _text_splitter) -> Tuple[List[str], List[dict]]:
    try:
        chunks = []
        metadata = []
        for context in tqdm(contexts, desc="Chunking documents"):
            doc_chunks = _text_splitter.split_text(context)
            for chunk in doc_chunks:
                chunks.append(chunk)
                metadata.append({"context": context})
        return chunks, metadata
    except Exception as e:
        logger.error(f"Error chunking documents: {e}")
        raise

# T·∫°o embeddings v·ªõi cache
@st.cache_data
def create_embeddings(chunks: List[str], model_path: str) -> np.ndarray:
    try:
        model = SentenceTransformer(model_path, device='cpu')
        gc.collect()
        chunk_size = 256
        batch_size = 4
        embeddings = []
        for i in tqdm(range(0, len(chunks), chunk_size), desc="Creating embeddings"):
            batch = chunks[i:i + chunk_size]
            batch_embeddings = model.encode(
                batch,
                batch_size=batch_size,
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            embeddings.append(batch_embeddings)
            gc.collect()
        embeddings = np.concatenate(embeddings, axis=0)
        return embeddings
    except Exception as e:
        logger.error(f"Error creating embeddings: {e}")
        raise

# Truy xu·∫•t t√†i li·ªáu
def custom_retrieval(query: str, documents: List[str], doc_embeddings: np.ndarray, metadata: List[dict], model, top_k: int = 5) -> List[Tuple[str, dict]]:
    try:
        start_time = time.time()
        query_embedding = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)
        index = faiss.IndexFlatIP(doc_embeddings.shape[1])
        index.add(doc_embeddings)
        _, top_indices = index.search(query_embedding, min(top_k, len(documents)))
        retrieved_docs = [(documents[i], metadata[i]) for i in top_indices[0]]
        elapsed_time = time.time() - start_time
        if elapsed_time < 1:
            time.sleep(1 - elapsed_time)
        return retrieved_docs
    except Exception as e:
        logger.error(f"Error in custom_retrieval: {e}")
        return []

# Tr·∫£ l·ªùi c√¢u h·ªèi
def answer_question(query: str, documents: List[str], doc_embeddings: np.ndarray, metadata: List[dict], model, generator, language: str = "vi") -> Tuple[str, List[Tuple[str, dict]]]:
    try:
        start_time = time.time()
        retrieved_docs = custom_retrieval(query, documents, doc_embeddings, metadata, model)
        if not retrieved_docs:
            return "Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan.", []

        context = " ".join([doc for doc, _ in retrieved_docs[:3]])
        prompt = (
            f"Ng·ªØ c·∫£nh: {context}\n"
            f"C√¢u h·ªèi: {query}\n"
            f"Tr·∫£ l·ªùi: H√£y vi·∫øt c√¢u tr·∫£ l·ªùi theo Ng·ªØ c·∫£nh"
        )

        outputs = generator(prompt, max_new_tokens=50, temperature=0.3, top_p=0.9, do_sample=True, num_beams=2)
        answer = outputs[0]["generated_text"].strip()

        elapsed_time = time.time() - start_time
        if elapsed_time < 1:
            time.sleep(1 - elapsed_time)
        return answer, retrieved_docs
    except Exception as e:
        logger.error(f"Error in answer_question: {e}")
        return "L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi.", []

# Chatbot RAG
def chatbot_rag(query_text: str = None, language: str = "vi") -> Tuple[str, str]:
    try:
        if not query_text:
            return "Vui l√≤ng nh·∫≠p c√¢u h·ªèi.", ""
        answer, sources = answer_question(query_text, st.session_state.chunks, st.session_state.embeddings, st.session_state.metadata, st.session_state.model, st.session_state.generator, language)
        source_text = sources[0][0] if sources else "Kh√¥ng t√¨m th·∫•y ngu·ªìn."
        return answer, source_text
    except Exception as e:
        logger.error(f"Error in chatbot_rag: {e}")
        return "L·ªói khi x·ª≠ l√Ω y√™u c·∫ßu.", ""

# Giao di·ªán Streamlit
def main():
    st.set_page_config(page_title="Chatbot", page_icon="ü§ñ", layout="wide")
    
    # Th√™m CSS t√πy ch·ªânh
    st.markdown("""
        <style>
        .stTextInput > div > div > input {
            border-radius: 10px;
            padding: 10px;
        }
        .stButton > button {
            background-color: #4CAF50;
            color: white;
            border-radius: 10px;
        }
        .stExpander {
            background-color: #f9f9f9;
            border-radius: 5px;
        }
        </style>
    """, unsafe_allow_html=True)

    st.title("Chatbot")
    st.write("H·ªèi b·∫•t k·ª≥ c√¢u h·ªèi n√†o!")

    # Kh·ªüi t·∫°o d·ªØ li·ªáu v√† m√¥ h√¨nh n·∫øu ch∆∞a c√≥
    if 'initialized' not in st.session_state:
        with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu v√† m√¥ h√¨nh..."):
            try:
                pdf_path = 'D:/5.11.2022/baitap_bosung_TIEULUAN MON HOC.pdf'
                contexts = load_documents(pdf_path)

                text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=20)
                st.session_state.chunks, st.session_state.metadata = chunk_documents(contexts, text_splitter)

                model_path = 'halong_embedding'
                st.session_state.embeddings = create_embeddings(st.session_state.chunks, model_path)

                model_name = "distilgpt2"
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model_llm = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    device_map="cpu",
                    low_cpu_mem_usage=True,
                    torch_dtype=torch.float32
                )
                st.session_state.generator = pipeline(
                    "text-generation",
                    model=model_llm,
                    tokenizer=tokenizer,
                    max_new_tokens=50,
                    temperature=0.3,
                    top_p=0.9,
                    do_sample=True,
                    num_beams=2,
                    return_full_text=False
                )
                st.session_state.model = SentenceTransformer(model_path, device='cpu')
                st.session_state.initialized = True
            except Exception as e:
                st.error(f"Kh·ªüi t·∫°o th·∫•t b·∫°i: {e}")
                logger.error(f"Initialization error: {e}")
                return

    # Kh·ªüi t·∫°o l·ªãch s·ª≠ tr√≤ chuy·ªán
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []

    # N√∫t x√≥a l·ªãch s·ª≠ tr√≤ chuy·ªán
    if st.button("X√≥a l·ªãch s·ª≠ tr√≤ chuy·ªán"):
        st.session_state.chat_history = []
        st.success("ƒê√£ x√≥a l·ªãch s·ª≠ tr√≤ chuy·ªán.")

    # Form nh·∫≠p c√¢u h·ªèi
    with st.form(key='question_form'):
        query = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:", placeholder="V√≠ d·ª•: N·ªôi dung ch√≠nh c·ªßa t√†i li·ªáu l√† g√¨?")
        submit_button = st.form_submit_button(label="G·ª≠i c√¢u h·ªèi")

    if submit_button and query:
        with st.spinner("ƒêang x·ª≠ l√Ω c√¢u h·ªèi..."):
            answer, source = chatbot_rag(query)
            st.session_state.chat_history.append({"question": query, "answer": answer, "source": source})
            st.subheader("C√¢u tr·∫£ l·ªùi:")
            st.markdown(f"**{answer}**")
            st.subheader("Ngu·ªìn:")
            st.write(source)

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán
    if st.session_state.chat_history:
        st.subheader("L·ªãch s·ª≠ tr√≤ chuy·ªán")
        for i, chat in enumerate(reversed(st.session_state.chat_history)):
            with st.expander(f"C√¢u h·ªèi {len(st.session_state.chat_history) - i}: {chat['question']}"):
                st.markdown(f"**Tr·∫£ l·ªùi**: {chat['answer']}")
                st.write(f"**Ngu·ªìn**: {chat['source']}")

if __name__ == "__main__":
    main()
