import streamlit as st
import os
import pandas as pd
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
import numpy as np
import gc
import faiss
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer
import re
import logging
import torch
from typing import List, Tuple
import time

# Thi·∫øt l·∫≠p logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# H√†m x·ª≠ l√Ω vƒÉn b·∫£n
def preprocess_text(text: str) -> str:
    try:
        return re.sub(r'\s+', ' ', text).strip()
    except Exception as e:
        logger.error(f"Error in preprocess_text: {e}")
        return text

# T·∫£i d·ªØ li·ªáu v·ªõi cache
@st.cache_data
def load_documents(contexts_path: str, questions_path: str, answers_path: str) -> Tuple[List[str], List[str], List[str]]:
    try:
        contexts, questions, answers = [], [], []
        for path, output_list in [(contexts_path, contexts), (questions_path, questions), (answers_path, answers)]:
            if not os.path.exists(path):
                raise FileNotFoundError(f"File not found: {path}")
            with open(path, 'r', encoding='utf-8') as file:
                lines = file.readlines()[:100]
                output_list.extend(preprocess_text(line.strip()) for line in lines if line.strip())
        if not (len(contexts) == len(questions) == len(answers)):
            raise ValueError("Mismatch in number of contexts, questions, and answers")
        return contexts, questions, answers
    except Exception as e:
        logger.error(f"Error loading documents: {e}")
        raise

# Chia nh·ªè vƒÉn b·∫£n v·ªõi cache
@st.cache_data
def chunk_documents(contexts: List[str], questions: List[str], answers: List[str], _text_splitter) -> Tuple[List[str], List[dict]]:
    try:
        chunks = []
        metadata = []
        for context, question, answer in tqdm(zip(contexts, questions, answers), total=len(contexts), desc="Chunking documents"):
            doc_chunks = _text_splitter.split_text(context)
            for chunk in doc_chunks:
                chunks.append(chunk)
                metadata.append({"context": context, "question": question, "answer": answer})
        return chunks, metadata
    except Exception as e:
        logger.error(f"Error chunking documents: {e}")
        raise

# T·∫°o embeddings v·ªõi cache
@st.cache_data
def create_embeddings(chunks: List[str], model_path: str) -> np.ndarray:
    try:
        model = SentenceTransformer(model_path, device='cpu')
        gc.collect()
        chunk_size = 256  # Gi·∫£m chunk_size ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ
        batch_size = 4    # Gi·∫£m batch_size ƒë·ªÉ t·ªëi ∆∞u CPU
        embeddings = []
        for i in tqdm(range(0, len(chunks), chunk_size), desc="Creating embeddings"):
            batch = chunks[i:i + chunk_size]
            batch_embeddings = model.encode(
                batch,
                batch_size=batch_size,
                show_progress_bar=False,
                convert_to_numpy=True,
                normalize_embeddings=True
            )
            embeddings.append(batch_embeddings)
            gc.collect()
        embeddings = np.concatenate(embeddings, axis=0)
        return embeddings
    except Exception as e:
        logger.error(f"Error creating embeddings: {e}")
        raise

# Truy xu·∫•t t√†i li·ªáu
def custom_retrieval(query: str, documents: List[str], doc_embeddings: np.ndarray, metadata: List[dict], model, top_k: int = 5) -> List[Tuple[str, dict]]:
    try:
        start_time = time.time()
        query_embedding = model.encode([query], convert_to_numpy=True, normalize_embeddings=True)
        index = faiss.IndexFlatIP(doc_embeddings.shape[1])
        index.add(doc_embeddings)
        _, top_indices = index.search(query_embedding, min(top_k, len(documents)))
        retrieved_docs = [(documents[i], metadata[i]) for i in top_indices[0]]
        elapsed_time = time.time() - start_time
        if elapsed_time < 1:
            time.sleep(1 - elapsed_time)
        return retrieved_docs
    except Exception as e:
        logger.error(f"Error in custom_retrieval: {e}")
        return []

# Tr·∫£ l·ªùi c√¢u h·ªèi
def answer_question(query: str, documents: List[str], doc_embeddings: np.ndarray, metadata: List[dict], model, generator, language: str = "vi") -> Tuple[str, List[Tuple[str, dict]]]:
    try:
        start_time = time.time()
        retrieved_docs = custom_retrieval(query, documents, doc_embeddings, metadata, model)
        if not retrieved_docs:
            return "Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan.", []

        context = " ".join([doc for doc, _ in retrieved_docs[:3]])
        example_prompt = ""
        for _, meta in retrieved_docs[:2]:
            example_prompt += f"V√≠ d·ª•: C√¢u h·ªèi: {meta['question']} Tr·∫£ l·ªùi: {meta['answer']}\n"

        prompt = (
            f"Ng·ªØ c·∫£nh: {context}\n"
            f"{example_prompt}"
            f"C√¢u h·ªèi: {query}\n"
            f"Tr·∫£ l·ªùi: Ch·ªâ cung c·∫•p c√¢u tr·∫£ l·ªùi ch√≠nh x√°c, kh√¥ng th√™m g√¨ kh√°c."
        )

        outputs = generator(prompt, max_new_tokens=50, temperature=0.3, top_p=0.9, do_sample=True, num_beams=2)
        answer = outputs[0]["generated_text"].strip()

        elapsed_time = time.time() - start_time
        if elapsed_time < 1:
            time.sleep(1 - elapsed_time)
        return answer, retrieved_docs
    except Exception as e:
        logger.error(f"Error in answer_question: {e}")
        return "L·ªói khi t·∫°o c√¢u tr·∫£ l·ªùi.", []

# Chatbot RAG
def chatbot_rag(query_text: str = None, language: str = "vi") -> Tuple[str, str]:
    try:
        if not query_text:
            return "Vui l√≤ng nh·∫≠p c√¢u h·ªèi.", ""
        answer, sources = answer_question(query_text, st.session_state.chunks, st.session_state.embeddings, st.session_state.metadata, st.session_state.model, st.session_state.generator, language)
        source_text = sources[0][0] if sources else "Kh√¥ng t√¨m th·∫•y ngu·ªìn."
        return answer, source_text
    except Exception as e:
        logger.error(f"Error in chatbot_rag: {e}")
        return "L·ªói khi x·ª≠ l√Ω y√™u c·∫ßu.", ""

# Giao di·ªán Streamlit
def main():
    st.set_page_config(page_title="hatbot", page_icon="ü§ñ", layout="wide")
    
    # Th√™m CSS t√πy ch·ªânh
    st.markdown("""
        <style>
        .stTextInput > div > div > input {
            border-radius: 10px;
            padding: 10px;
        }
        .stButton > button {
            background-color: #4CAF50;
            color: white;
            border-radius: 10px;
        }
        .stExpander {
            background-color: #f9f9f9;
            border-radius: 5px;
        }
        </style>
    """, unsafe_allow_html=True)

    st.title("Chatbot")
    st.write("H·ªèi b·∫•t k·ª≥ c√¢u h·ªèi n√†o!")

    # Kh·ªüi t·∫°o d·ªØ li·ªáu v√† m√¥ h√¨nh n·∫øu ch∆∞a c√≥
    if 'initialized' not in st.session_state:
        with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu v√† m√¥ h√¨nh..."):
            try:
                contexts_path = './data/viquad.contexts'
                questions_path = './data/viquad.questions'
                answers_path = './data/viquad.answers'
                contexts, questions, answers = load_documents(contexts_path, questions_path, answers_path)

                text_splitter = RecursiveCharacterTextSplitter(chunk_size=256, chunk_overlap=20)
                st.session_state.chunks, st.session_state.metadata = chunk_documents(contexts, questions, answers, text_splitter)
                # st.success(f"ƒê√£ chia nh·ªè th√†nh {len(st.session_state.chunks)} ƒëo·∫°n vƒÉn b·∫£n.")

                model_path = 'halong_embedding'
                st.session_state.embeddings = create_embeddings(st.session_state.chunks, model_path)

                model_name = "distilgpt2"
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                model_llm = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    device_map="cpu",
                    low_cpu_mem_usage=True,
                    torch_dtype=torch.float32
                )
                st.session_state.generator = pipeline(
                    "text-generation",
                    model=model_llm,
                    tokenizer=tokenizer,
                    max_new_tokens=50,
                    temperature=0.3,
                    top_p=0.9,
                    do_sample=True,
                    num_beams=2,
                    return_full_text=False
                )
                st.session_state.model = SentenceTransformer(model_path, device='cpu')
                st.session_state.initialized = True
                # st.success("Kh·ªüi t·∫°o m√¥ h√¨nh v√† d·ªØ li·ªáu th√†nh c√¥ng!")
            except Exception as e:
                st.error(f"Kh·ªüi t·∫°o th·∫•t b·∫°i: {e}")
                logger.error(f"Initialization error: {e}")
                return

    # Kh·ªüi t·∫°o l·ªãch s·ª≠ tr√≤ chuy·ªán
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []

    # N√∫t x√≥a l·ªãch s·ª≠ tr√≤ chuy·ªán
    if st.button("X√≥a l·ªãch s·ª≠ tr√≤ chuy·ªán"):
        st.session_state.chat_history = []
        st.success("ƒê√£ x√≥a l·ªãch s·ª≠ tr√≤ chuy·ªán.")

    # Form nh·∫≠p c√¢u h·ªèi
    with st.form(key='question_form'):
        query = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:", placeholder="V√≠ d·ª•: T√™n g·ªçi n√†o ƒë∆∞·ª£c Ph·∫°m VƒÉn ƒê·ªìng s·ª≠ d·ª•ng khi l√†m Ph√≥ ch·ªß nhi·ªám c∆° quan Bi·ªán s·ª± x·ª© t·∫°i Qu·∫ø L√¢m?")
        submit_button = st.form_submit_button(label="G·ª≠i c√¢u h·ªèi")

    if submit_button and query:
        with st.spinner("ƒêang x·ª≠ l√Ω c√¢u h·ªèi..."):
            answer, source = chatbot_rag(query)
            st.session_state.chat_history.append({"question": query, "answer": answer, "source": source})
            st.subheader("C√¢u tr·∫£ l·ªùi:")
            st.markdown(f"**{answer}**")
            st.subheader("Ngu·ªìn:")
            st.write(source)

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ tr√≤ chuy·ªán
    if st.session_state.chat_history:
        st.subheader("L·ªãch s·ª≠ tr√≤ chuy·ªán")
        for i, chat in enumerate(reversed(st.session_state.chat_history)):
            with st.expander(f"C√¢u h·ªèi {len(st.session_state.chat_history) - i}: {chat['question']}"):
                st.markdown(f"**Tr·∫£ l·ªùi**: {chat['answer']}")
                st.write(f"**Ngu·ªìn**: {chat['source']}")

if __name__ == "__main__":
    main()